{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df= pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "#df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n",
    "        # Features and Labels\n",
    "df['label'] = df['class'].map({'ham': 0, 'spam': 1})\n",
    "X = df['message']\n",
    "y = df['label']\n",
    "        \n",
    "# Extract Feature With CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(X) # Fit the Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "message = 'FREE'\n",
    "data = [message]\n",
    "vect = cv.transform(data).toarray()\n",
    "my_prediction = clf.predict(vect)\n",
    "my_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.json') as f:\n",
    "    raw_train = json.load(f)\n",
    "with open('test.json') as f:\n",
    "    raw_test = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1945</td>\n",
       "      <td>negative</td>\n",
       "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957</td>\n",
       "      <td>negative</td>\n",
       "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1969</td>\n",
       "      <td>negative</td>\n",
       "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973</td>\n",
       "      <td>negative</td>\n",
       "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1975</td>\n",
       "      <td>negative</td>\n",
       "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id sentiment                                               text\n",
       "0  1945  negative  Досудебное расследование по факту покупки ЕНПФ...\n",
       "1  1957  negative  Медики рассказали о состоянии пострадавшего му...\n",
       "2  1969  negative  Прошел почти год, как железнодорожным оператор...\n",
       "3  1973  negative  По итогам 12 месяцев 2016 года на территории р...\n",
       "4  1975  negative  Астана. 21 ноября. Kazakhstan Today - Агентств..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame.from_dict(raw_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1945</td>\n",
       "      <td>negative</td>\n",
       "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1957</td>\n",
       "      <td>negative</td>\n",
       "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1969</td>\n",
       "      <td>negative</td>\n",
       "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973</td>\n",
       "      <td>negative</td>\n",
       "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1975</td>\n",
       "      <td>negative</td>\n",
       "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id sentiment                                               text  label\n",
       "0  1945  negative  Досудебное расследование по факту покупки ЕНПФ...    0.0\n",
       "1  1957  negative  Медики рассказали о состоянии пострадавшего му...    0.0\n",
       "2  1969  negative  Прошел почти год, как железнодорожным оператор...    0.0\n",
       "3  1973  negative  По итогам 12 месяцев 2016 года на территории р...    0.0\n",
       "4  1975  negative  Астана. 21 ноября. Kazakhstan Today - Агентств...    0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "import string\n",
    "\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "\"\"\"\n",
    "stemmed = []\n",
    "Xp =[]\n",
    "for i in range(len(X)):\n",
    "    for word in X[i].split(' '): \n",
    "        s= stemmer.stem(word)\n",
    "        if s not in stopwords.words('russian'):\n",
    "            stemmed.append(s)\n",
    "            #proc = [x for x in stemmed if x not in stopwords.words('russian')]\n",
    "            st =' '.join(word for word in stemmed)\n",
    "            st =st.translate(str.maketrans('', '', string.punctuation))\n",
    "    Xp.append(st)\n",
    "\n",
    "    stemmed = []\n",
    "    st=''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(Xp, 'xp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Xp =pd.Series(Xp).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature With CountVectorizer\n",
    "cv1 = CountVectorizer()\n",
    "countvecX = open('xp.pkl','rb')\n",
    "Xp = joblib.load(countvecX)\n",
    "#countvec = open('cv.pkl','rb')\n",
    "#cv = joblib.load(countvec)\n",
    "Xp = cv1.fit_transform(Xp) # Fit the Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xp, y, test_size=0.33, random_state=10)\n",
    "\n",
    "#model = open('model.pkl','rb')\n",
    "#clf = joblib.load(model)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7187385405207187"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract Feature With CountVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "countvecX = open('xp.pkl','rb')\n",
    "Xp = joblib.load(countvecX)\n",
    "#countvec = open('cv.pkl','rb')\n",
    "#cv = joblib.load(countvec)\n",
    "Xs = tfidf.fit_transform(Xp) # Fit the Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.33, random_state=10)\n",
    "\n",
    "#model = open('model.pkl','rb')\n",
    "#clf = joblib.load(model)\n",
    "clf2 = SGDClassifier()\n",
    "clf2.fit(X_train,y_train)\n",
    "clf2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(tfidf, 'tfidf.pkl')\n",
    "#from sklearn.externals import joblib\n",
    "#joblib.dump(clf2, 'model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= \"В ОАЭ состоялись переговоры между казахстанской и эмиратской стороной по поводу дальнейшей совместной работы и поддержки. По итогу встречи было решено создать совместную инвестиционную платформу двух стран, сообщает NUR.KZ со ссылкой на официальный сайт премьер-министра\"\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [t]\n",
    "vect = tfidf.transform(data).toarray()\n",
    "my_prediction = clf2.predict(vect)\n",
    "my_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =tfidf.vocabulary_\n",
    "x\n",
    "#{k: v for k, v in sorted(x.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['состоялись', 'официальный', 'между', 'ссылкой', 'решено',\n",
       "       'создать', 'платформу', 'казахстанской', 'стороной', 'поводу'],\n",
       "      dtype='<U161')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "string= \"В ОАЭ состоялись переговоры между казахстанской и эмиратской стороной по поводу дальнейшей совместной работы и поддержки. По итогу встречи было решено создать совместную инвестиционную платформу двух стран, сообщает NUR.KZ со ссылкой на официальный сайт премьер-министра\"\n",
    "response = tfidf.transform([string])\n",
    "\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 10\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "stemmed = []\n",
    "for word in t.split(' '):\n",
    "    stemmed.append(stemmer.stem(word))\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = [x for x in stemmed if x not in stopwords.words('russian')]\n",
    "proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "countvecX = open('xp.pkl','rb')\n",
    "Xp = joblib.load(countvecX)\n",
    "tfidf = TfidfVectorizer()\n",
    "Xs = tfidf.fit_transform(Xp)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xs, y, test_size=0.33, random_state=10)\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message= \"Главный герой — 27-летний Илья Горюнов, семь лет отсидевший в тюрьме по ложному обвинению в распространении наркотиков. Когда Илья выходит на свободу, он понимает, что прежняя жизнь, по которой он тосковал, разрушена, и вернуться к ней он больше не сможет. Хотя он не собирался мстить человеку, \"\n",
    "stemmer = RussianStemmer()\n",
    "stemmed = []        \n",
    "for word in message.split(' '):\n",
    "    stemmed.append(stemmer.stem(word))\n",
    "    proc = [x for x in stemmed if x not in stopwords.words('russian')]  \n",
    "    st =' '.join(word for word in proc)\n",
    "    st= st.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "data = [st]\n",
    "vect = tfidf.transform(data).toarray()\n",
    "my_prediction = clf.predict(vect)\n",
    "\n",
    "my_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#response = tfidf.transform([str])\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(vect.flatten()[::-1])\n",
    "\n",
    "n = 10\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "te = \"\"\"Two Travellers, walking in the noonday sun, sought the shade of a widespreading tree to rest. As they lay looking up among the pleasant leaves, they saw that it was a Plane Tree.\n",
    "\n",
    "\"How useless is the Plane!\" said one of them. \"It bears no fruit whatever, and only serves to litter the ground with leaves.\"\n",
    "\n",
    "\"Ungrateful creatures!\" said a voice from the Plane Tree. \"You lie here in my cooling shade, and yet you say I am useless! Thus ungratefully, O Jupiter, do men receive their blessings!\"\n",
    "\n",
    "Our best blessings are often the least appreciated.\"\"\"\n",
    "\n",
    "tfs = tfidf.fit_transform(te.split(\" \"))\n",
    "string = 'tree cat travellers fruit jupiter'\n",
    "response = tfidf.transform([string])\n",
    "\n",
    "feature_array = np.array(tfidf1.get_feature_names())\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "top_n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
